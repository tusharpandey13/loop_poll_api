{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "poll_data_df = pd.read_csv(\"store status.csv\").sort_values(by='timestamp_utc')\n",
    "store_timing_data_df = pd.read_csv(\"Menu hours.csv\")\n",
    "tz_data_df = pd.read_csv(\"bq-results-20230125-202210-1674678181880.csv\")\n",
    "\n",
    "\n",
    "report_cols = ['store_id', 'uptime_last_hour', 'uptime_last_day', 'uptime_last_week', 'downtime_last_hour', 'downtime_last_day', 'downtime_last_week']\n",
    "\n",
    "def map_status(status):\n",
    "    return 1.0 if status == \"active\" else 0.0\n",
    "   \n",
    "def empty_output(store_id):\n",
    "    return pd.DataFrame([{\n",
    "        'store_id': store_id,\n",
    "        'uptime_last_hour': 0,\n",
    "        'uptime_last_day': 0, \n",
    "        'uptime_last_week': 0, \n",
    "        'downtime_last_hour': 60,\n",
    "        'downtime_last_day': 24,\n",
    "        'downtime_last_week': 168\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize store hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_store_timing_df(store_timing_data_df, tz_data_df):\n",
    "    \n",
    "    # Convert 'start_time_local' and 'end_time_local' columns to datetime objects\n",
    "    store_timing_data_df['start_time_local'] = pd.to_datetime(store_timing_data_df['start_time_local'], format='%H:%M:%S')\n",
    "    store_timing_data_df['end_time_local'] = pd.to_datetime(store_timing_data_df['end_time_local'], format='%H:%M:%S')\n",
    "\n",
    "    # Merge DataFrames on 'store_id'\n",
    "    merged_df = pd.merge(store_timing_data_df, tz_data_df, on='store_id')\n",
    "\n",
    "    # Convert 'start_time_local' and 'end_time_local' columns to UTC using 'timezone_str'\n",
    "    merged_df['start_time_utc'] = merged_df.apply(lambda row: row['start_time_local'].tz_localize(row['timezone_str']).tz_convert('UTC'), axis=1)\n",
    "    merged_df['end_time_utc'] = merged_df.apply(lambda row: row['end_time_local'].tz_localize(row['timezone_str']).tz_convert('UTC'), axis=1)\n",
    "\n",
    "    # Ensure both 'start_time_local' and 'start_time_utc' are tz-aware\n",
    "    merged_df['start_time_local'] = merged_df['start_time_local'].dt.tz_localize('UTC')\n",
    "    merged_df['end_time_local'] = merged_df['end_time_local'].dt.tz_localize('UTC')\n",
    "\n",
    "    # Calculate delta based on time change\n",
    "    merged_df['delta'] = (merged_df['start_time_utc'] - merged_df['start_time_local']).dt.days\n",
    "\n",
    "    # Calculate 'day_utc' based on the original 'day' column and delta\n",
    "    merged_df['day_utc'] = merged_df['day'] + merged_df['delta']\n",
    "\n",
    "    # Drop unnecessary columns if needed\n",
    "    result_df = merged_df.drop(['start_time_local', 'end_time_local', 'timezone_str', 'delta', 'day'], axis=1)\n",
    "\n",
    "    result_df['start_time_utc'] = result_df['start_time_utc'].dt.time\n",
    "    result_df['end_time_utc'] = result_df['end_time_utc'].dt.time\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample the data to hour boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(df):\n",
    "    # Set the timestamp column as the index\n",
    "    df.set_index('timestamp_utc', inplace=True)\n",
    "\n",
    "    # Resample data to hourly intervals and fill missing values\n",
    "    df_resampled = df.resample('H').ffill()\n",
    "\n",
    "    df_resampled[\"status\"] = df_resampled[\"status\"].apply(map_status)\n",
    "\n",
    "    return df_resampled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolate missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(df, df_resampled):\n",
    "    # Define a function to determine the majority status in each hour\n",
    "    def majority_status(series):\n",
    "        return series.mode().iloc[0] if not series.empty else None\n",
    "\n",
    "    # Apply the majority_status function to get the status for each hour\n",
    "    result = df_resampled['status'].rolling(window=2).apply(majority_status)\n",
    "    # Create a new DataFrame with the results\n",
    "    result_df = pd.DataFrame({'status': result})\n",
    "\n",
    "    # Display the resulting DataFrame\n",
    "    result_df = result_df.iloc[1:]\n",
    "\n",
    "    if result_df.empty:\n",
    "        raise Exception('empty')  \n",
    "    \n",
    "    result_df['status'].iloc[0] = map_status(df['status'].iloc[0])\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the poll data according to store hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_timings(df, store_id, store_timing_df):\n",
    "    # Extract store_id, start_time_utc, end_time_utc from result_df\n",
    "    store_info = store_timing_df[store_timing_df['store_id'] == store_id][['start_time_utc', 'end_time_utc', 'day_utc']]\n",
    "    skip_flag = 0\n",
    "    if(store_info.empty):\n",
    "        return df\n",
    "    \n",
    "    # Filter rows based on store_id and day_utc\n",
    "    df[\"day_utc\"] = df[\"timestamp_utc\"].dt.dayofweek\n",
    "    return df[\n",
    "            (df[\"day_utc\"].isin(store_info['day_utc'].values)) &\n",
    "            (df[\"timestamp_utc\"].dt.time >= store_info['start_time_utc'].values[0]) &\n",
    "            (df[\"timestamp_utc\"].dt.time <= store_info['end_time_utc'].values[0])\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data for a single store_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subset(df, store_id, store_timing_df):\n",
    "    df[\"timestamp_utc\"] = pd.to_datetime(df[\"timestamp_utc\"], format=\"%Y-%m-%d %H:%M:%S.%f %Z\", errors=\"coerce\").fillna(pd.to_datetime(df[\"timestamp_utc\"], format=\"%Y-%m-%d %H:%M:%S %Z\", errors=\"coerce\"))    \n",
    "\n",
    "    df = filter_timings(df, store_id, store_timing_df)\n",
    "    \n",
    "    if df.empty:\n",
    "        return empty_output(store_id)\n",
    "\n",
    "    df_resampled = resample(df)\n",
    "\n",
    "    if df.empty:\n",
    "        return empty_output(store_id)\n",
    "\n",
    "    # print(df_resampled)\n",
    "    df_final = fill_missing(df, df_resampled)\n",
    "    # print(df_final)\n",
    "\n",
    "    if df.empty:\n",
    "        return empty_output(store_id)\n",
    "    \n",
    "    return generate_status_summary(df_final, store_id)\n",
    "\n",
    "\n",
    "def compute_report(store_id, store_timing_df):\n",
    "    try:\n",
    "        tmpdf = poll_data_df[poll_data_df[\"store_id\"] == store_id].drop(\"store_id\", axis=1)\n",
    "        return process_subset(tmpdf, store_id, store_timing_df)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate uptimes and downtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_status_summary(df, store_id):\n",
    "    # Calculate uptime and downtime for the last hour \n",
    "    last_hour = df['status'].iloc[-1]\n",
    "\n",
    "    # Calculate uptime and downtime for the whole of the last calendar day\n",
    "    last_day = df['status'].loc[df.index.normalize() == df.index[-1].normalize()].values\n",
    "\n",
    "    # Calculate uptime and downtime for the whole of the last calendar week\n",
    "    last_week = df['status'].loc[df.index >= (df.index[-1] - pd.DateOffset(weeks=1))].values\n",
    "\n",
    "    return pd.DataFrame([{\n",
    "        'store_id': store_id,\n",
    "        'uptime_last_hour': last_hour,\n",
    "        'uptime_last_day': sum(last_day == 1), \n",
    "        'uptime_last_week': sum(last_week == 1), \n",
    "        'downtime_last_hour': 60 - last_hour,\n",
    "        'downtime_last_day': sum(last_day == 0),\n",
    "        'downtime_last_week': sum(last_week == 0)\n",
    "    }])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the result on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dt/dgs17w15537dzp29rf5spjpm0000gp/T/ipykernel_37967/463642298.py:8: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  result_df =  pd.concat([summary, result_df], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>uptime_last_hour</th>\n",
       "      <th>uptime_last_day</th>\n",
       "      <th>uptime_last_week</th>\n",
       "      <th>downtime_last_hour</th>\n",
       "      <th>downtime_last_day</th>\n",
       "      <th>downtime_last_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1217505942626861817</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3820046627859013224</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8810005333786545600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "      <td>37</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3575054060857067667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>19</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4349618685772968066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>19</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14041</th>\n",
       "      <td>3480743953780628156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "      <td>169</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14042</th>\n",
       "      <td>7863591736472240852</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "      <td>163</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14043</th>\n",
       "      <td>7212727908964340554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>149</td>\n",
       "      <td>59.0</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14044</th>\n",
       "      <td>5159242830714471711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>149</td>\n",
       "      <td>59.0</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14045</th>\n",
       "      <td>9080834095465144554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "      <td>154</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14046 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  store_id  uptime_last_hour uptime_last_day uptime_last_week  \\\n",
       "0      1217505942626861817               1.0               4                4   \n",
       "1      3820046627859013224               1.0               4                4   \n",
       "2      8810005333786545600               1.0              19               37   \n",
       "3      3575054060857067667               0.0               0                0   \n",
       "4      4349618685772968066               0.0               0                0   \n",
       "...                    ...               ...             ...              ...   \n",
       "14041  3480743953780628156               1.0              19              169   \n",
       "14042  7863591736472240852               1.0              19              163   \n",
       "14043  7212727908964340554               1.0               4              149   \n",
       "14044  5159242830714471711               1.0               4              149   \n",
       "14045  9080834095465144554               1.0              19              154   \n",
       "\n",
       "       downtime_last_hour downtime_last_day downtime_last_week  \n",
       "0                    59.0                 0                  0  \n",
       "1                    59.0                 0                  0  \n",
       "2                    59.0                 0                  0  \n",
       "3                    60.0                19                 37  \n",
       "4                    60.0                19                 37  \n",
       "...                   ...               ...                ...  \n",
       "14041                59.0                 0                  0  \n",
       "14042                59.0                 0                  6  \n",
       "14043                59.0                 7                 20  \n",
       "14044                59.0                 7                 20  \n",
       "14045                59.0                 0                 15  \n",
       "\n",
       "[14046 rows x 7 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "result_df = pd.DataFrame(columns=report_cols)\n",
    "\n",
    "store_timing_df = get_store_timing_df(store_timing_data_df, tz_data_df)\n",
    "\n",
    "for store_id in poll_data_df['store_id'].unique():\n",
    "    summary = compute_report(store_id, store_timing_df)\n",
    "    result_df =  pd.concat([summary, result_df], ignore_index=True)\n",
    "result_df\n",
    "\n",
    "# result_df = compute_report(8349337405318481717, store_timing_df)\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
